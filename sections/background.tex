\section{Background}

%The agent each step t interact with the environment and receive a reward. 

The interaction between the agent and the environment is described  with the
Markov Decision Process (MDP) formalism. A MDP is a tupla $\langle \mathcal{S}, 
\mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$ where $\mathcal{S}$ is
the set of states, $\mathcal{A}$ is the set of actions, $\mathcal{P}$ is the 
state transition probability, $\mathcal{R}$ is the reward function (???) and 
$\gamma$ is the discount factor determining the agent's horizon.

The main goal of an agent is to interact with the environment maximizing the future rewards. In order to do this it estimates the action value function $Q$ that computes how much expected return come from a state $s$ given an action $a$: $Q(s, a) = \max_\pi E[R_t | s_t = s, a_t = a, \pi]$ where $\pi$ is a policy, that is a function that maps states to actions.


Unluckily in a real context the MDP is unknown or its representation is too big
(ex. game Go). To face this issue numerous methods were proposed. They are 
classified as \textit{model-free} methods because they didn't need any prior knowledge and they learn only from sampling.


\subsection{Q-learning}

One method that tackles the previous issue is the \textbf{Q-learning} algorithm: a \textit{model-free} \textit{off-policy} method \cite{Watkins1992}. 
\textit{Off-policy} means that the Q-learning learns using two policy. The first one is the target policy $\pi$, that follows the \textit{greedy} strategy $a = \max_a Q(s, a; \theta)$, that is selecting always the action that will return the maximum reward.
The second one is the \textit{behavior policy} $\mu$ that ensures an exploration of the states space, typically an \textit{$\epsilon$-greedy} strategy. \textit{$\epsilon$-greedy} is a greedy strategy only with probability $1 - \epsilon$ and selects a random action with probability $\epsilon$.

The Q-learning first initialize arbitrarily the $Q(s,a)$ except for the terminal state where $Q$ is initialize to 0.
The agent from the initial state $S_0$ starts the iteration identified by a step $t$. Inside this loop the agent choose an action $A_t$ from $S_t$ (current state) using the \textit{behavior policy}. Then it takes the action $A_t$ and observe from the environment the immediate reward $R_{t+1}$ and the resulting state $S_{t+1}$ with which it estimates the action-value function through the following update rule:
\begin{equation}
	Q(S_t, A_t) = Q(S_t, A_t) + \alpha (Y^Q_t - Q(S_t, A_t))
\end{equation}
Where $\alpha$ is a scalar step size and the target $Y^Q_t$ is defined as
\begin{equation}
	Y^Q_t \equiv R_{t+1} + \gamma \max_{a} Q(S_{t+1}, a)
\end{equation}

Finally $S'$ is assigned to $S$. The iteration goes on until $S$ is a terminal state.

\subsection{Deep Q Network algorithm}
\label{subsec:DQN}

%(What do the authors want to do?)
In the~\cite{DBLP:journals/corr/MnihKSGAWR13} work a new successful result using Q-learning was obtained implementing the \textbf{Deep Q Network} (\textbf{DQN}) \textbf{algorithm}. This DQN trained with Q-learning has learnt successfully to play 7 Atari 2600 games only from high-dimensional sensory input.

DQN is a multi-layered Convolution Neural Network $Q(s, \cdotp; \theta)$, where $\theta$ are the parameters, that approximate the action-value function $Q$. It takes a 84x84 gray-scale image as input representing the state $s$, and it outputs one predicted $Q$-value for each valid action. The target used is:
\begin{equation}
Y^{DQN}_t \equiv R_{t+1} + \gamma \max_{a} Q(S_{t+1}, a; \theta_t)
\end{equation}

%(What were the general issues?)
%(How do they solve them?)
One important issue faced is that the agent has to learn from noisy data highly correlate. In order to break these correlations they successfully applied the \textit{experience replay} technique~\cite{Lin:1992:RLR:168871}. It consists in building a data-set of samples \textit{replay memory}. At each step a sample $e_t = (s_t, a_t, r_t, s_{t+1})$ is store in the memory overwriting the most old sample. At each step of the Q-learning algorithm the DQN is updated sampling uniformly from this memory. Using \textit{experience replay} implies a greater data efficiency since each sample is potentially used in many weight updates and it avoids oscillations or divergence in the parameters of the network.

%(What is the result?)
In the seven games used as testing environment the DQN algorithm opportunely tuned has given state-of-the-art results in six of them.




\subsection{Double Q-learning}
\label{subsec:DoubleQlearning}

It is proved that Q-learning overestimates the action-value function~\cite{NIPS2010_3964}. This is caused by the approximation of the maximal estimator $\max_a Q(S_{t+1}, a)$ that is a bias estimator of $max_a E \{ Q(S_{t+1}, a) \}$, this bias is cumulative at each update. In order to remove this effect the \textbf{Double Q-learning} algorithm has been proposed~\cite{NIPS2010_3964}.

The main idea of Double Q-learning is to use a double estimator: keeping two action-value function $Q^A$ and $Q^B$. At each step only one of two function is updated and in the update instead of using the $Q^A(S{t+1}, a^*) = \max_a Q(S_{t+1}, a)$ where $a^* = \max_a Q^A(S_{t+1}, a)$ it is used $Q^B(S_{t+1}, a^*)$.
Van Hasselt (2010) has proved mathematically and in small environments that the Double Q-learning is more accurate than Q-learning and sometimes underestimate.

\subsection{Double Deep Q Network}

%(What do the authors want to do?)
%(What is the result?)
In the article \textit{Human-level control through deep reinforcement learning}~\cite{Mnih2015} and successively in \textit{Deep Reinforcement Learning with Double Q-learning}~\cite{Hasselt:2016:DRL:3016100.3016191} the DQN algorithm is enchanted with the using of Double Q-learning algorithm described above. The final algorithm, called \textbf{Double Deep Q Network} (\textbf{DDQN}) \textbf{algorithm}, showed an important result for the Deep Reinforcement Learning field. The agent trains itself and learns to play better than human player in (???) of 57 Atari 2600 games.

%(How does the algorithm work?)

%(What were the general issues?)
%(How do they solve them?)
The issue resolved with Double Q-learning was the high overestimation caused by Q-learning. \citeauthor{Hasselt:2016:DRL:3016100.3016191}~\shortcite{Hasselt:2016:DRL:3016100.3016191} shown that Double Q-learning improve the estimation involving an improving of the general performance of the DQN algorithm.


