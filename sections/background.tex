\section{Background}

%The agent each step t interact with the environment and receive a reward. 

The interaction between the agent and the environment is described  with the
Markov Decision Process (MDP) formalism. A MDP is a tupla $\langle \mathcal{S}, 
\mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$ where $\mathcal{S}$ is
the set of states, $\mathcal{A}$ is the set of actions, $\mathcal{P}$ is the 
state transition probability, $\mathcal{R}$ is the reward function (???) and 
$\gamma$ is the discount factor.

The main goal of an agent is to interact with the environment maximizing the future rewards. In order to do this it estimates the action value function $Q$ that computes how much expected return come from a state $s$ given an action $a$: $Q(s, a) = \max_\pi E[R_t | s_t = s, a_t = a, \pi]$ where $\pi$ is a policy: a function that maps states to actions.


Unluckily in a real context the MDP is unknown or its representation is too big
(ex. game Go). To face this issue numerous methods were proposed, they are 
called \textit{model free} because they didn't need any prior knowledge and they learn only from sampling.


\subsection{Q-learning}

One method that tackles the previous issue is the \textbf{Q-learning} algorithm: a model free off policy method \cite{Watkins1992}. 
\textit{Off-policy} means that the Q-learning learning use two policy. The first one is the target policy $\pi$, that follows the \textit{greedy} strategy $a = \max_a Q(s, a; \theta)$, that is selecting always the action that will return the maximum reward.
The second one is the \textit{behavior policy} $\mu$ that ensures an exploration of the states space, typically an \textit{$\epsilon$-greedy} strategy. \textit{$\epsilon$-greedy} is a greedy strategy only with probability $1 - \epsilon$ and selects a random action with probability $\epsilon$.

It estimate the action-value function using the following update rule:
\begin{equation}
	Q(s,a) = Q(s, a) + \alpha (r + \gamma \max_{a'} Q(s', a') - Q(s, a))
\end{equation}

(How Q-learning works???)

\subsection{Q-learning and Deep Q Networks}

In the recent advance with Deep Learning seems natural to try to apply these techniques in the Reinforcement Learning. Since the amazing result of Deep Neural Networks on raw sensory data, the authors of~\cite{DBLP:journals/corr/MnihKSGAWR13} have thought to use a Deep Q network (DQN) as function approximator of the action-value function.

A DQN is a multi-layered neural network that maps a state $s$ to a vector of action values. It is defined as $Q(s, \cdotp; \theta)$ where $\theta$ are the parameters of the network. 

One important issue faced is that the agent has to learn from noisy data highly correlate. In order to break these correlation they successfully applied the \textit{experience replay} technique~\cite{Lin:1992:RLR:168871}. It consists in building a data-set samples called \textit{replay memory}. At each step a sample $e_t = (s_t, a_t, r_t, s_{t+1})$ is store in the memory overwriting the most old sample. Q-learning updates are done taken minibatches from this memory.

DeepMind with this work has shown as a DQN algorithm 


\subsection{Double Q-learning}

It is proved that Q-learning overestimates the action-value function 
~\cite{NIPS2010_3964}. This is caused by the approximation of the maximal estimator $\max_a Q(S_{t+1}, a)$ that is a bias estimator of $max_a E \{ Q(S_{t+1}, a) \}$, this bias is cumulative at each update. In order to remove this effect Van Hasselt proposed the \textbf{Double Q-learning} algorithm~\cite{NIPS2010_3964}. 
The solution is to apply a double estimator to Q-learning, keeping two action-value function $Q^A$ and $Q^B$, at each step update only one of two function, in the update instead of use the $Q^A(S{t+1}, a^*) = \max_a Q(S_{t+1}, a)$ where $a^* = \max_a Q^A(S_{t+1}, a)$, it use $Q^B(S_{t+1}, a^*)$.
Van Hasselt has shown in small experiments that the Double Q-learning is more accurate than Q-learning and sometimes underestimate.

\subsection{Double Q-learning and Deep Q Networks}

