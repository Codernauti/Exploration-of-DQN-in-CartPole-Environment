\section{Experiments}

\subsection{Environment}
Our testbed is done in the \textit{CartPole-v0} environment available in the Gym toolkit \cite{Gym}.
In this toy problem a pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity. It was invented for having pseudo-standard benchmark problem from the field of control theory and artificial neural networks for designing and testing controllers on complex and unstable non-linear system.

The problem is defined \textit{solving} if the agent get average reward of 195.0 over 100 consecutive trials. We used this as a parameter for evaluating our trained models.
 
\subsubsection{States space}
A state of the agent is described through a vector of 4 features: the  \textit{Cart Position}, \textit{Cart Velocity}, \textit{Pole Angle} and \textit{Pole Velocity At Tip} (which can be interpreted on how fast the pole is falling) .

\subsubsection{Actions space}
Using the previous observations, the agent needs to decide on one of two possible actions: apply a left or right force to the cart. Force is described as discrete number: 0 for the left, 1 for the right.


%One of the main challenges was to convert the continuous, 4-dimensional input space to a discrete space. 

%For the Q-learning algorithm, less states we have, smaller the Q-table will be and the less steps the agent will need to properly learn its values. 

\subsubsection{Episode termination}
An episode terminates when the pole angle is more than $\pm12$ or cart position is more than $\pm2.4$ or episode reaches the $200^{th}$ step.

\subsubsection{Reward}
%Normally in games, the reward directly relates to the score of the game.
The reward is 1 for every step taken, including the termination step. Since the maximum number of step for episode is 200 the maximum reward for episode is 200.


\subsection{Neural Network Architecture}

For our Deep Q Network we built two different architectures: the most simple consists of an input layer of 4 units, the state features, a single fully connected hidden layer of 64 units (\textit{DQN Shallow}) with ReLU as activation function and two units, the possible actions, for the output layer. The second one is the previous network but with an additional hidden layer of 32 units with ReLU as activation function before the output layer (\textit{DQN deep}). %The third configuration keeps the input and output layers of the first one, between them there are three fully connected hidden layers of 32, 16 and 8 units respectively \textit{(DQN more deep)}. 
The last one was created for comparing the difference between deep and shallow neural networks.

\subsection{General Settings}

For the settings of the DQN we started from the hyper-parameters used by \citeauthor{Hasselt:2016:DRL:3016100.3016191} \shortcite{Hasselt:2016:DRL:3016100.3016191}. Since the smaller complexity of the \textit{CartPole} environment and our limited hardware resources, we set the \textit{experience replay} dataset size  to 10000 instead of 1 Million and we updated the target network parameters every 1000 steps ($\tau = 1000$). As loss function we used \textit{Mean Squared Error} minimized using the RMSprop optimizer with mini-batches of sizes 32.

Each agent runs the training for a total of 3600 episodes using \textit{$\epsilon$-greedy} with $\epsilon$ initialized to 1 decreasing linearly for 10.000 steps and then fixed at 0.01 for all the next steps.

We trained different agents with the DQN algorithm \cite{Mnih2015} and the Double DQN algorithm \cite{Hasselt:2016:DRL:3016100.3016191}.

\begin{table}
	\centering
	\begin{tabular}{|l|r|}
		\hline
		\multicolumn{1}{|c|}{\textbf{Hyper-parameter}} &
		\multicolumn{1}{|c|}{\textbf{Value}}        \\
		\hline
		Minibatch size & 32 \\
		Replay memory size & 10000 \\
		Target network update frequency $\tau$ & 1000 \\
		Discount factor $\gamma$ & 0.99 \\
		Learning rate $\alpha$ & 0.00025 \\
		Gradient momentum & 0.9 \\
		Initial exploration $\epsilon$ & 1 \\
		Final exploration $\epsilon$ & 0.01 \\
		Final exploration step & 10000 \\
		\hline
	\end{tabular}
	
	\caption{The hyper-parameters used in all the configurations.}
	\label{tab:my-hyperparams}
\end{table}

\subsection{Configurations}

We built 4 configuration that are all the combinations of neural network architectures and DQN algorithms, they are: \textit{DQN shallow} (also referred to as DQN), \textit{DQN deep}, \textit{Double DQN shallow} (DDQN) and \textit{Double DQN deep}. Each of them has set the hyper-parameters shown in the Table~\ref{tab:my-hyperparams}.

Aware that more runs are required, specially in the reinforcement learning field \cite{DBLP:journals/corr/abs-1709-06560}, we tested the configurations with only 3 different seeds due to time constraints for a total of 12 runs.

% Comparing seed and stochasticity

\subsection{Evaluation}

We evaluated the models taken every 100 episodes of an execution, each execution lasted 3600 episodes for a total of 36 models for each configuration. Each model is evaluated in an agent without Q-learning for 100 episodes. The evaluation consists of measuring the mean and the standard deviation of the reward in 100 episodes.



