\section{Introduction}

In the last decade Reinforcement Learning (RL) is attracting more and more interest of machine learning and artificial intelligence communities.

The main objective of the research is to develop agents who should be able to choose the actions to be performed, given the current state of the environment in which it is located, with the aim of maximizing the total reward.

More recently, with the leveraging on the rapid growth in the amount of the annotated data and the great improvements in the strengths of graphics processor units, the research on  Deep Learning (DL) has been emerged and led to very good performance on a variety of problems, such as visual recognition, speech recognition and natural language processing \cite{AdvancesCNN}.

Reinforcement Learning algorithms that incorporate deep learning is enabling to scale problems considered previously intractable. Some of the most amazing results have been obtained by Googleâ€™s DeepMind, who has trained agents capable to defeat Go champion, Le Sedol, in march 2016, four games to one, or able to achieve the performance of expert humans in playing numerous Atari video games learning directly from pixels.

OpenAI's bot has beaten the world's top professionals at 1v1 matches of Dota 2 under standard tournament rules, after learning the rules of the game from scratch by self-play.
They released Gym toolkit \cite{Gym}, a collection of test problems designed for testing and developing reinforcement learning algorithms, including 57 games of Atari 2600 and the MuJoCo physics engine, where simulated robot agents can be controlled by the position and velocity of each joint. 

The goal of this work is to implement the DQN algorithm comparing the first version with the Double DQN version. Following initially the experiments performed in Atari environment, specifically the Pong game, we soon clashed with computational and time limits due to the huge number of parameters of network and memory required. In short, for replicate the experiments cited we would have required weeks of computation time and a GPU with more than 8 Gigabytes of memory. We have therefore chosen to analyze a relatively simpler problem: the \textit{CartPole}, provided by Gym.

This paper is organized as follows: the Background section presents an overview of the deep reinforcement learning methods. The Experiments section introduces the environment and the task of the \textit{CartPole} problem while the Results Discussion section contains an analysis of the experiments results. In Future Work, we propose some of the possible interesting changes on our experiments settings. Finally, the Conclusion containing our thoughts about Deep Reinforcement Learning after this exploration work.
