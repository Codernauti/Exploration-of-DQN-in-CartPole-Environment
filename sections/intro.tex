\section{Introduction}

\noindent 
Reinforcement Learning (RL), in the last few years is attracting more and more interest of machine learning and artificial intelligence communities, as a powerful tool for solving complex sequential decision making problems in control theory.

More recently, with the leveraging on the rapid growth in the amount of the annotated data and the great improvements in the strengths of graphics processor units, the research on  Deep Learning (DL) has been emerged and led to very good performance on a variety of problems, such as visual recognition, speech recognition and natural language processing.
\\Reinforcement algorithms that incorporate deep learning is enabling to scale to problems considered previously intractable, at least in an acceptable time frame.

Some of the most amazing results have been obtained by Googleâ€™s DeepMind, who has trained agents capable to defeated Go champion, Le Sedol, in march 2016, four games to one **, or as well as human experts playing numerous Atari video games **.
\\OpenAI's bot has beaten the world's top professionals at 1v1 matches of Dota 2 ** under standard tournament rules. The bot learned the game from scratch by self-play.
They released Gym toolkit \cite{Gym}, a collection of environments/problems designed for testing and developing reinforcement learning algorithms, included 57 games of Atari 2600 and the MuJoCo ** physics engine, where simulated robot agents can be controlled by the position and velocity of each joint. 
\\\\
In this report we show an overview of the methods that concern the RL, specifically, the methods that involves the Q-learning algorithm, come up to the most recent DQN algorithm that combine it to deep neural network.
\\We then show the experiments performed on CartPole environment, a "small" challenge contained in Gym, that allowed us to deal with the main issues related to implementation of simplified DQN.
