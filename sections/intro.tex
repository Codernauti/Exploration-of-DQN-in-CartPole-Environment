\section{Introduction}

In the last decade Reinforcement Learning (RL) is attracting more and more interest of machine learning and artificial intelligence communities.

The main objective of the RL research is developing agents which should be able to choose the actions maximizing the total reward, given only the current state of the environment in which it is located.

More recently, with the leveraging on the rapid growth in the amount of the annotated data and the great improvements in the strengths of graphics processor units, the research in Deep Learning (DL) has been emerged and led to very good performance on a variety of problems, such as Visual Recognition, Speech Recognition and Natural Language Processing \cite{AdvancesCNN}.

Deep Learning in addition to Reinforcement Learning enables to scale problems considered previously intractable. Some of the most amazing results have been obtained by Googleâ€™s DeepMind that has trained agents capable to defeat Go champion, Le Sedol, in march 2016, four games to one, or able to achieve the performance of expert humans in playing numerous Atari video games learning directly from pixels. Also OpenAI obtained a great success with its bot that has beaten the world's top professionals at 1v1 matches of Dota 2 under standard tournament rules, after learning the rules of the game from scratch by self-play.

In order to increase the community around RL OpenAI released Gym toolkit \cite{Gym}, a collection of test problems designed for testing and developing reinforcement learning algorithms, including 57 games of Atari 2600 and the MuJoCo physics engine, where simulated robot agents can be controlled by the position and velocity of each joint. 

The goal of this work is firstly to understand the DQN algorithm issues and the solution adopted, secondly to implement the DQN algorithm comparing the first version with the Double DQN version. Following initially the experiments performed in Atari environment, specifically the Pong game. \Auth~soon clashed with computational and time limits due to the huge number of parameters of network and memory required. In short, for replicate the experiments cited \auth~would have required weeks of computation time and a GPU with more than 8 Gigabytes of memory. \Auth~have therefore chosen to analyze a relatively simpler problem: the \textit{CartPole}, provided by Gym.\\

This paper is organized as follows: the Background section presents an overview of the deep reinforcement learning methods. The Experiments section introduces the environment and the task of the \textit{CartPole} problem while the Results Discussion section contains an analysis of the experiments results. In Future Work, \auth~propose some of the possible interesting changes on the experiments settings. Finally, the Conclusion containing \authpp~thoughts about Deep Reinforcement Learning after this exploration work.
