\section{Conclusions}

\Auth~have presented how Q-learning, and successively Double Q-learning was combined with Deep Neural Network creating the DQN algorithm. 
An algorithm that achieved state-of-the-art performance in the \textit{Atari Emulator Environment} and keeps improving with the contributes of deep reinforcement learning community~\cite{DBLP:journals/corr/abs-1710-02298}.

\Auth~implemented the DQN algorithm and \auth~tried to explore its potential in the toy control environment \textit{CartPole}. Without the need of tuning \auth~showed that DQN was able to solve the problem with discrete results. \Auth~showed also that the introduction of the Double DQN improves a bit the performance and comparing to DQN algorithm reduce the overestimation as \citeauthor{Hasselt:2016:DRL:3016100.3016191} \shortcite{Hasselt:2016:DRL:3016100.3016191} demonstrated.

Despite the issues faced with data and debugging, with this experiment \auth~have seen how DQN algorithm can be considered a base for carrying on future researches in this area. 