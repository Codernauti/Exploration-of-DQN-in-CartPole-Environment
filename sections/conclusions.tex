\section{Conclusions}

\Auth~have presented how Q-learning, and successively Double Q-learning was combined with Deep Neural Network creating the DQN algorithm. 
An algorithm that achieved state-of-the-art performance in the \textit{Atari Emulator Environment} and keeps improving with the contributes of deep reinforcement learning community~\cite{DBLP:journals/corr/abs-1710-02298}.

\Auth~implemented the DQN algorithm and \auth~tried to explore its potential in the toy control environment \textit{CartPole}. Without the need of tuning \auth~showed that DQN was able to solve the problem with good results. \Auth~showed also that the introduction of the Double DQN does not improves the performance and comparing to DQN algorithm in \textit{CartPole} environment it does not reduce the overestimation. Furthermore \Auth~showed that a \textit{deep} neural network in the \textit{CartPole} context improve the result in the short run: few episodes for solving the problem, but in the long run it suffers of overestimation and overfitting. Anyway for a clearer point of view more tests on different neural network architecture are necessary.

Despite the issues faced with data and debugging, with this experiment \auth~have seen how DQN algorithm can be considered a base for carrying on future researches in this area. 