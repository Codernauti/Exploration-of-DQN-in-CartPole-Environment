\section{Conclusions}

We have presented how Q-learning, and successively Double Q-learning was combined with Deep Neural Network creating the DQN algorithm. 
An algorithm that achieved state-of-the-art performance in the \textit{Atari Emulator Environment} and keeps improving with the contributes of deep reinforcement learning community~\cite{DBLP:journals/corr/abs-1710-02298}.

We implemented the DQN algorithm and we tried to explore its potential in the toy control environment \textit{CartPole}. Without the need of tuning we showed that DQN was able to solve the problem with discrete results. We shows also that the introduction of the Double DQN improve a little the performance and comparing to DQN algorithm reduce the overestimation as \citeauthor{Hasselt:2016:DRL:3016100.3016191} \shortcite{Hasselt:2016:DRL:3016100.3016191} demonstrated.

Despite the issues faced with data and debugging, with this experiment we have seen how DQN algorithm can be considered a base for carrying on future researches in this area. 