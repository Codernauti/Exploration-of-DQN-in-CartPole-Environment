
\section{Future Works}

\subsection{Continuous time}

The used environment of the \textit{CartPole} limits the steps to 200 for each episode. It might be interesting to perform the same experiments on the same environment with no step limits. In order to see if a continuous environment can encourage the Q-learning to learn smarter policies.

\subsection{Reward function redesign}

Another interesting test could be to redesign the reward function. A reward that decreases as much as the agent is far from the centre might encourage the agent to learn a policy in which it's better to stay in the middle.

\subsection{Longer Time Execution}

\Authpp~tests is obviously limited. The cited references for DQN and Double DQN executed for a total of 200 Million of frames, that is 50 Million of steps, while the maximum reached by \authpp~tests was about 1 Million. Increasing the number of episodes and therefore the number of steps could show better the behaviour of the Q-value in \textit{CartPole} environment.

\subsection{More Neural Network Architectures}
\Auth~showed that the \textit{deep} neural network suffers of overestimation in the long run, neither the Double Q-learning reduces this behaviour. In order to deepen this aspect more architectures have to be tested and compared.